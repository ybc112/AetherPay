"""
AIæ¨¡å‹è‡ªåŠ¨æ›´æ–°ç³»ç»Ÿ - ä¿æŒé¢„è¨€æœºå‡†ç¡®æ€§
é€šè¿‡å®šæœŸé‡è®­ç»ƒå’Œå¢é‡å­¦ä¹ ä¿æŒæ¨¡å‹æ—¶æ•ˆæ€§
"""

import os
import sys
import time
import json
import logging
import sqlite3
import schedule
import subprocess
from datetime import datetime, timedelta
from typing import Dict, List, Tuple

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('model_updater.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('ModelUpdater')

class ModelLifecycleManager:
    """AIæ¨¡å‹ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨"""

    def __init__(self):
        self.model_dir = 'saved_models'
        self.db_path = 'aether_oracle.db'
        self.performance_history = {}

        # æ¨¡å‹æ›´æ–°ç­–ç•¥é…ç½®
        self.config = {
            'max_model_age_days': 3,          # æ¨¡å‹æœ€å¤§å¹´é¾„ï¼ˆå¤©ï¼‰
            'min_accuracy_threshold': 0.85,    # æœ€ä½å‡†ç¡®ç‡é˜ˆå€¼
            'retrain_interval_hours': 24,      # å®šæœŸé‡è®­ç»ƒé—´éš”ï¼ˆå°æ—¶ï¼‰
            'incremental_update_hours': 6,     # å¢é‡æ›´æ–°é—´éš”ï¼ˆå°æ—¶ï¼‰
            'performance_check_hours': 1,      # æ€§èƒ½æ£€æŸ¥é—´éš”ï¼ˆå°æ—¶ï¼‰
            'min_data_points': 1000,          # é‡è®­ç»ƒæœ€å°‘æ•°æ®ç‚¹
            'rolling_window_days': 30,        # æ»šåŠ¨çª—å£å¤©æ•°
        }

    def check_model_health(self, pair: str) -> Dict:
        """æ£€æŸ¥æ¨¡å‹å¥åº·çŠ¶æ€"""
        pair_key = pair.replace('/', '_')
        model_file = f"{self.model_dir}/{pair_key}_lgb_model.txt"
        metadata_file = f"{self.model_dir}/{pair_key}_metadata.json"

        health_report = {
            'pair': pair,
            'exists': False,
            'age_days': None,
            'last_accuracy': None,
            'current_accuracy': None,
            'needs_update': False,
            'update_reason': []
        }

        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
        if not os.path.exists(model_file):
            health_report['needs_update'] = True
            health_report['update_reason'].append('æ¨¡å‹ä¸å­˜åœ¨')
            return health_report

        health_report['exists'] = True

        # æ£€æŸ¥æ¨¡å‹å¹´é¾„
        try:
            with open(metadata_file, 'r') as f:
                metadata = json.load(f)

            trained_at = datetime.fromisoformat(metadata['trained_at'])
            age_days = (datetime.now() - trained_at).days
            health_report['age_days'] = age_days
            health_report['last_accuracy'] = metadata['metrics'].get('r2', 0)

            # å¹´é¾„æ£€æŸ¥
            if age_days > self.config['max_model_age_days']:
                health_report['needs_update'] = True
                health_report['update_reason'].append(f'æ¨¡å‹è¿‡æ—§ï¼ˆ{age_days}å¤©ï¼‰')

        except Exception as e:
            logger.error(f"è¯»å–æ¨¡å‹å…ƒæ•°æ®å¤±è´¥ {pair}: {e}")
            health_report['needs_update'] = True
            health_report['update_reason'].append('å…ƒæ•°æ®é”™è¯¯')

        # æ£€æŸ¥å½“å‰æ€§èƒ½
        current_accuracy = self.evaluate_model_performance(pair)
        health_report['current_accuracy'] = current_accuracy

        if current_accuracy and current_accuracy < self.config['min_accuracy_threshold']:
            health_report['needs_update'] = True
            health_report['update_reason'].append(f'å‡†ç¡®ç‡ä¸‹é™ï¼ˆ{current_accuracy:.3f}ï¼‰')

        return health_report

    def evaluate_model_performance(self, pair: str) -> float:
        """è¯„ä¼°æ¨¡å‹å½“å‰æ€§èƒ½"""
        try:
            # è·å–æœ€è¿‘24å°æ—¶çš„é¢„æµ‹å’Œå®é™…æ•°æ®
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥è®°å½•é¢„æµ‹å€¼å¹¶å¯¹æ¯”
            cursor.execute("""
                SELECT AVG(price) as avg_price,
                       MIN(price) as min_price,
                       MAX(price) as max_price,
                       COUNT(*) as count
                FROM exchange_rates
                WHERE pair = ?
                AND timestamp > datetime('now', '-1 day')
            """, (pair,))

            result = cursor.fetchone()
            conn.close()

            if result and result[3] > 100:  # è‡³å°‘100ä¸ªæ•°æ®ç‚¹
                # ç®€å•çš„æ³¢åŠ¨ç‡ä½œä¸ºæ€§èƒ½æŒ‡æ ‡ï¼ˆå®é™…åº”è¯¥ç”¨é¢„æµ‹vså®é™…ï¼‰
                avg_price = result[0]
                volatility = (result[2] - result[1]) / avg_price if avg_price else 0

                # æ³¢åŠ¨ç‡è¶Šä½ï¼Œè®¤ä¸ºæ¨¡å‹è¶Šç¨³å®šï¼ˆç®€åŒ–å¤„ç†ï¼‰
                estimated_accuracy = max(0.5, 1 - volatility * 10)
                return estimated_accuracy

        except Exception as e:
            logger.error(f"è¯„ä¼°æ¨¡å‹æ€§èƒ½å¤±è´¥ {pair}: {e}")

        return None

    def retrain_model(self, pair: str, mode: str = 'full') -> bool:
        """é‡è®­ç»ƒæ¨¡å‹"""
        try:
            logger.info(f"å¼€å§‹{mode}è®­ç»ƒ {pair} æ¨¡å‹...")

            # æ„å»ºè®­ç»ƒå‘½ä»¤
            cmd = [
                'python3',
                'models/train_model_optimized.py',
                '--pair', pair
            ]

            if mode == 'incremental':
                cmd.append('--incremental')
            elif mode == 'fast':
                cmd.append('--fast-mode')

            # æ‰§è¡Œè®­ç»ƒ
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=600  # 10åˆ†é’Ÿè¶…æ—¶
            )

            if result.returncode == 0:
                logger.info(f"âœ… {pair} æ¨¡å‹è®­ç»ƒæˆåŠŸ")

                # è®°å½•è®­ç»ƒå†å²
                self.record_training_history(pair, mode, 'success')
                return True
            else:
                logger.error(f"âŒ {pair} æ¨¡å‹è®­ç»ƒå¤±è´¥: {result.stderr}")
                self.record_training_history(pair, mode, 'failed')
                return False

        except Exception as e:
            logger.error(f"è®­ç»ƒæ¨¡å‹å¼‚å¸¸ {pair}: {e}")
            return False

    def record_training_history(self, pair: str, mode: str, status: str):
        """è®°å½•è®­ç»ƒå†å²"""
        history = {
            'pair': pair,
            'timestamp': datetime.now().isoformat(),
            'mode': mode,
            'status': status
        }

        # ä¿å­˜åˆ°æ–‡ä»¶
        history_file = 'training_history.json'
        try:
            if os.path.exists(history_file):
                with open(history_file, 'r') as f:
                    all_history = json.load(f)
            else:
                all_history = []

            all_history.append(history)

            # åªä¿ç•™æœ€è¿‘100æ¡è®°å½•
            all_history = all_history[-100:]

            with open(history_file, 'w') as f:
                json.dump(all_history, f, indent=2)

        except Exception as e:
            logger.error(f"è®°å½•è®­ç»ƒå†å²å¤±è´¥: {e}")

    def get_active_pairs(self) -> List[str]:
        """è·å–æ´»è·ƒçš„è´§å¸å¯¹"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            # è·å–æœ€è¿‘æœ‰æ•°æ®çš„è´§å¸å¯¹
            cursor.execute("""
                SELECT pair, COUNT(*) as count
                FROM exchange_rates
                WHERE timestamp > datetime('now', '-7 days')
                GROUP BY pair
                HAVING count > ?
                ORDER BY count DESC
            """, (self.config['min_data_points'],))

            pairs = [row[0] for row in cursor.fetchall()]
            conn.close()

            return pairs

        except Exception as e:
            logger.error(f"è·å–æ´»è·ƒè´§å¸å¯¹å¤±è´¥: {e}")
            return []

    def update_all_models(self):
        """æ›´æ–°æ‰€æœ‰éœ€è¦æ›´æ–°çš„æ¨¡å‹"""
        logger.info("="*60)
        logger.info("ğŸ”„ å¼€å§‹æ¨¡å‹æ›´æ–°æ£€æŸ¥...")

        active_pairs = self.get_active_pairs()
        logger.info(f"æ´»è·ƒè´§å¸å¯¹: {len(active_pairs)}ä¸ª")

        update_summary = {
            'checked': 0,
            'updated': 0,
            'failed': 0,
            'skipped': 0
        }

        for pair in active_pairs:
            update_summary['checked'] += 1

            # æ£€æŸ¥æ¨¡å‹å¥åº·çŠ¶æ€
            health = self.check_model_health(pair)

            logger.info(f"\n{pair} å¥åº·æ£€æŸ¥:")
            logger.info(f"  å­˜åœ¨: {health['exists']}")
            logger.info(f"  å¹´é¾„: {health['age_days']}å¤©")
            logger.info(f"  ä¸Šæ¬¡å‡†ç¡®ç‡: {health['last_accuracy']:.3f}" if health['last_accuracy'] else "  ä¸Šæ¬¡å‡†ç¡®ç‡: N/A")
            logger.info(f"  å½“å‰å‡†ç¡®ç‡: {health['current_accuracy']:.3f}" if health['current_accuracy'] else "  å½“å‰å‡†ç¡®ç‡: N/A")

            if health['needs_update']:
                logger.info(f"  âš ï¸ éœ€è¦æ›´æ–°: {', '.join(health['update_reason'])}")

                # å†³å®šæ›´æ–°æ¨¡å¼
                if not health['exists'] or health['age_days'] > 7:
                    mode = 'full'  # å®Œå…¨é‡è®­ç»ƒ
                else:
                    mode = 'fast'  # å¿«é€Ÿæ›´æ–°

                # æ‰§è¡Œæ›´æ–°
                if self.retrain_model(pair, mode):
                    update_summary['updated'] += 1
                else:
                    update_summary['failed'] += 1
            else:
                logger.info(f"  âœ… æ¨¡å‹å¥åº·ï¼Œæ— éœ€æ›´æ–°")
                update_summary['skipped'] += 1

            # é¿å…è¿‡è½½
            time.sleep(2)

        # æ˜¾ç¤ºæ›´æ–°æ€»ç»“
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š æ›´æ–°æ€»ç»“:")
        logger.info(f"  æ£€æŸ¥: {update_summary['checked']}ä¸ª")
        logger.info(f"  æ›´æ–°: {update_summary['updated']}ä¸ª")
        logger.info(f"  å¤±è´¥: {update_summary['failed']}ä¸ª")
        logger.info(f"  è·³è¿‡: {update_summary['skipped']}ä¸ª")
        logger.info("="*60)

    def incremental_learning(self, pair: str):
        """å¢é‡å­¦ä¹ ï¼ˆåœ¨çº¿å­¦ä¹ ï¼‰"""
        logger.info(f"æ‰§è¡Œ {pair} å¢é‡å­¦ä¹ ...")

        # TODO: å®ç°å¢é‡å­¦ä¹ é€»è¾‘
        # 1. åŠ è½½ç°æœ‰æ¨¡å‹
        # 2. è·å–æ–°æ•°æ®
        # 3. å¢é‡æ›´æ–°æ¨¡å‹å‚æ•°
        # 4. ä¿å­˜æ›´æ–°åçš„æ¨¡å‹

        pass

    def start_scheduler(self):
        """å¯åŠ¨è‡ªåŠ¨æ›´æ–°è°ƒåº¦å™¨"""
        logger.info("ğŸš€ å¯åŠ¨æ¨¡å‹è‡ªåŠ¨æ›´æ–°ç³»ç»Ÿ")
        logger.info(f"é…ç½®:")
        logger.info(f"  - å®Œæ•´æ£€æŸ¥: æ¯{self.config['retrain_interval_hours']}å°æ—¶")
        logger.info(f"  - æ€§èƒ½ç›‘æ§: æ¯{self.config['performance_check_hours']}å°æ—¶")
        logger.info(f"  - æ¨¡å‹æœ€å¤§å¹´é¾„: {self.config['max_model_age_days']}å¤©")
        logger.info(f"  - æœ€ä½å‡†ç¡®ç‡: {self.config['min_accuracy_threshold']}")

        # ç«‹å³æ‰§è¡Œä¸€æ¬¡
        self.update_all_models()

        # è®¾ç½®å®šæœŸä»»åŠ¡
        schedule.every(self.config['retrain_interval_hours']).hours.do(self.update_all_models)
        schedule.every(self.config['performance_check_hours']).hours.do(self.check_all_performance)

        # æ¯å¤©å‡Œæ™¨2ç‚¹æ‰§è¡Œå®Œæ•´æ›´æ–°
        schedule.every().day.at("02:00").do(self.daily_maintenance)

        logger.info("\nâœ… è°ƒåº¦å™¨å·²å¯åŠ¨ï¼ŒæŒ‰Ctrl+Cåœæ­¢")

        while True:
            try:
                schedule.run_pending()
                time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
            except KeyboardInterrupt:
                logger.info("\nğŸ‘‹ æ¨¡å‹æ›´æ–°ç³»ç»Ÿå·²åœæ­¢")
                break
            except Exception as e:
                logger.error(f"è°ƒåº¦å™¨é”™è¯¯: {e}")
                time.sleep(60)

    def check_all_performance(self):
        """æ£€æŸ¥æ‰€æœ‰æ¨¡å‹æ€§èƒ½"""
        logger.info("ğŸ“Š æ‰§è¡Œæ€§èƒ½æ£€æŸ¥...")

        pairs = self.get_active_pairs()
        alerts = []

        for pair in pairs:
            health = self.check_model_health(pair)
            if health['current_accuracy'] and health['current_accuracy'] < 0.8:
                alerts.append(f"{pair}: å‡†ç¡®ç‡ {health['current_accuracy']:.3f}")

        if alerts:
            logger.warning("âš ï¸ æ€§èƒ½è­¦æŠ¥:")
            for alert in alerts:
                logger.warning(f"  - {alert}")

    def daily_maintenance(self):
        """æ¯æ—¥ç»´æŠ¤ä»»åŠ¡"""
        logger.info("ğŸ”§ æ‰§è¡Œæ¯æ—¥ç»´æŠ¤...")

        # 1. æ¸…ç†æ—§æ•°æ®
        self.cleanup_old_data()

        # 2. å®Œæ•´æ¨¡å‹æ›´æ–°
        self.update_all_models()

        # 3. ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
        self.generate_performance_report()

    def cleanup_old_data(self):
        """æ¸…ç†æ—§æ•°æ®"""
        try:
            # è°ƒç”¨æ¸…ç†è„šæœ¬
            subprocess.run(['python3', 'cleanup_old_data.py', '--days', '30'])
            logger.info("âœ… æ—§æ•°æ®æ¸…ç†å®Œæˆ")
        except Exception as e:
            logger.error(f"æ¸…ç†æ•°æ®å¤±è´¥: {e}")

    def generate_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'models': {}
        }

        pairs = self.get_active_pairs()
        for pair in pairs:
            health = self.check_model_health(pair)
            report['models'][pair] = health

        # ä¿å­˜æŠ¥å‘Š
        report_file = f"performance_report_{datetime.now().strftime('%Y%m%d')}.json"
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2)

        logger.info(f"ğŸ“„ æ€§èƒ½æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='AIæ¨¡å‹ç”Ÿå‘½å‘¨æœŸç®¡ç†')
    parser.add_argument('--check', help='æ£€æŸ¥ç‰¹å®šæ¨¡å‹å¥åº·çŠ¶æ€', metavar='PAIR')
    parser.add_argument('--update', help='ç«‹å³æ›´æ–°ç‰¹å®šæ¨¡å‹', metavar='PAIR')
    parser.add_argument('--update-all', action='store_true', help='æ›´æ–°æ‰€æœ‰æ¨¡å‹')
    parser.add_argument('--daemon', action='store_true', help='ä»¥å®ˆæŠ¤è¿›ç¨‹æ¨¡å¼è¿è¡Œ')

    args = parser.parse_args()

    manager = ModelLifecycleManager()

    if args.check:
        health = manager.check_model_health(args.check)
        print(json.dumps(health, indent=2))
    elif args.update:
        manager.retrain_model(args.update)
    elif args.update_all:
        manager.update_all_models()
    elif args.daemon:
        manager.start_scheduler()
    else:
        # é»˜è®¤è¿è¡Œè°ƒåº¦å™¨
        manager.start_scheduler()

if __name__ == '__main__':
    main()
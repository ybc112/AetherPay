"""
å¹³è¡¡ç‰ˆè®­ç»ƒè„šæœ¬ - é€Ÿåº¦ä¸å‡†ç¡®ç‡çš„æœ€ä½³å¹³è¡¡
é¢„æœŸï¼š2-3åˆ†é’Ÿè®­ç»ƒï¼ŒRÂ²>0.85
"""

import numpy as np
import pandas as pd
import lightgbm as lgb
from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from scipy.stats import uniform, randint
import json
import os
import sys
from datetime import datetime
import logging
import warnings
import argparse

warnings.filterwarnings('ignore')

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from data.data_preprocessor_optimized import OptimizedDataPreprocessor

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class BalancedModelTrainer:
    """å¹³è¡¡é€Ÿåº¦ä¸å‡†ç¡®ç‡çš„è®­ç»ƒå™¨"""

    def __init__(self, model_dir='./saved_models'):
        self.model_dir = model_dir
        self.preprocessor = OptimizedDataPreprocessor()
        os.makedirs(model_dir, exist_ok=True)

    def train_balanced(self, pair='ETH/USDT', days=7):
        """
        å¹³è¡¡è®­ç»ƒ - 2-3åˆ†é’Ÿå®Œæˆï¼Œå‡†ç¡®ç‡>0.85
        """
        logger.info(f"\n{'='*60}")
        logger.info(f"âš–ï¸ å¼€å§‹å¹³è¡¡è®­ç»ƒ {pair} æ¨¡å‹")
        logger.info(f"ğŸ“… è®­ç»ƒæ•°æ®: {days}å¤©")
        logger.info(f"{'='*60}\n")

        start_time = datetime.now()

        # Step 1: åŠ è½½æ•°æ®
        logger.info("ğŸ“¥ åŠ è½½æ•°æ®...")
        rates_df, market_df = self.preprocessor.load_training_data(pair, days)

        if rates_df.empty:
            raise ValueError(f"âŒ {pair} æ²¡æœ‰å¯ç”¨æ•°æ®")

        logger.info(f"âœ… åŸå§‹æ•°æ®: {len(rates_df)}æ¡è®°å½•")

        # Step 2: åˆ›å»ºç‰¹å¾ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
        logger.info("ğŸ”§ åˆ›å»ºç‰¹å¾...")
        features_df = self.preprocessor.create_features(rates_df, market_df)
        features_df = self.preprocessor.add_external_features(features_df)

        # Step 3: å‡†å¤‡MLæ•°æ®ï¼ˆå‘é‡åŒ–ç‰ˆæœ¬ - å¿«100å€ï¼ï¼‰
        logger.info("ğŸ“Š å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆå‘é‡åŒ–ï¼‰...")
        X, y, timestamps, feature_cols = self.preprocessor.prepare_ml_data_vectorized(
            features_df,
            target_col='avg_price',
            lookback=12,  # 12å°æ—¶çª—å£
            lookahead=6   # é¢„æµ‹6å°æ—¶å
        )

        if len(X) == 0:
            raise ValueError("æ•°æ®ä¸è¶³")

        logger.info(f"âœ… è®­ç»ƒæ•°æ®: {len(X)}æ ·æœ¬ Ã— {X.shape[1]}ç‰¹å¾")

        # Step 4: ç‰¹å¾é€‰æ‹©ï¼ˆå¿«é€Ÿç‰ˆï¼‰
        logger.info("ğŸ” å¿«é€Ÿç‰¹å¾é€‰æ‹©...")
        X_selected, selected_features = self._quick_feature_selection(X, y, feature_cols, n_features=80)

        # Step 5: äº¤å‰éªŒè¯è®­ç»ƒ
        logger.info("ğŸ¯ å¼€å§‹è®­ç»ƒ...")
        tscv = TimeSeriesSplit(n_splits=3)
        model, best_score, best_params = self._train_with_cv(X_selected, y, tscv)

        logger.info(f"ğŸ“Š æœ€ä½³RÂ²: {best_score:.4f}")

        # Step 6: æœ€ç»ˆè¯„ä¼°
        final_metrics = self._evaluate_model(model, X_selected, y, tscv)

        # Step 7: ä¿å­˜æ¨¡å‹
        metadata = self._save_model(
            pair, model, selected_features,
            days, best_score, best_params, final_metrics
        )

        total_time = (datetime.now() - start_time).total_seconds()

        logger.info(f"\n{'='*60}")
        logger.info(f"âœ… è®­ç»ƒå®Œæˆï¼")
        logger.info(f"â±ï¸  æ€»è€—æ—¶: {total_time:.1f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
        logger.info(f"ğŸ“ˆ RÂ² Score: {best_score:.4f}")
        logger.info(f"ğŸ“Š RMSE: {final_metrics['rmse']:.2f}")
        logger.info(f"ğŸ“Š MAE: {final_metrics['mae']:.2f}")
        logger.info(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {self.model_dir}")
        logger.info(f"{'='*60}\n")

        return metadata

    def _quick_feature_selection(self, X, y, feature_names, n_features=80):
        """å¿«é€Ÿç‰¹å¾é€‰æ‹© - åªç”¨30æ£µæ ‘"""

        # è®­ç»ƒå°æ¨¡å‹è·å–ç‰¹å¾é‡è¦æ€§
        selector = lgb.LGBMRegressor(
            n_estimators=30,  # å‡å°‘åˆ°30æ£µæ ‘
            learning_rate=0.1,
            max_depth=5,
            num_leaves=31,
            random_state=42,
            n_jobs=-1,
            verbosity=-1
        )

        selector.fit(X, y)

        # è·å–ç‰¹å¾é‡è¦æ€§
        importances = selector.feature_importances_

        # ç”Ÿæˆå±•å¹³åçš„ç‰¹å¾åï¼ˆå¦‚æœéœ€è¦ï¼‰
        n_original_features = len(feature_names)
        n_total_features = X.shape[1]
        lookback = n_total_features // n_original_features  # è®¡ç®—å®é™…çš„lookback

        if n_total_features != n_original_features:
            # ç”Ÿæˆå±•å¹³åçš„ç‰¹å¾å
            expanded_features = []
            for i in range(lookback):
                for feat in feature_names:
                    expanded_features.append(f'{feat}_t-{lookback-i}')
            feature_names = expanded_features[:n_total_features]  # ç¡®ä¿é•¿åº¦åŒ¹é…

        # é€‰æ‹©Top Nç‰¹å¾
        n_features = min(n_features, len(importances))  # ä¸è¶…è¿‡å®é™…ç‰¹å¾æ•°
        indices = np.argsort(importances)[-n_features:]
        X_selected = X[:, indices]

        # ç¡®ä¿ç´¢å¼•ä¸è¶Šç•Œ
        selected_features = []
        for idx in indices:
            if idx < len(feature_names):
                selected_features.append(feature_names[idx])
            else:
                selected_features.append(f'feature_{idx}')

        return X_selected, selected_features

    def _train_with_cv(self, X, y, tscv):
        """ä½¿ç”¨å°‘é‡å‚æ•°æœç´¢è®­ç»ƒ"""

        # ç²¾ç®€çš„å‚æ•°æœç´¢ç©ºé—´
        param_distributions = {
            'num_leaves': [31, 50, 70],
            'learning_rate': [0.03, 0.05, 0.1],
            'feature_fraction': [0.8, 0.9],
            'bagging_fraction': [0.8, 0.9],
            'max_depth': [5, 7, 9]
        }

        base_model = lgb.LGBMRegressor(
            n_estimators=150,
            random_state=42,
            n_jobs=-1,
            verbosity=-1
        )

        # éšæœºæœç´¢ï¼ˆåªæœç´¢10æ¬¡ï¼‰
        random_search = RandomizedSearchCV(
            base_model,
            param_distributions,
            n_iter=10,  # å‡å°‘åˆ°10æ¬¡
            cv=tscv,
            scoring='r2',
            n_jobs=-1,
            verbose=0,
            random_state=42
        )

        random_search.fit(X, y)

        return random_search.best_estimator_, random_search.best_score_, random_search.best_params_

    def _evaluate_model(self, model, X, y, tscv):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        all_predictions = []
        all_actuals = []

        for train_idx, test_idx in tscv.split(X):
            X_test = X[test_idx]
            y_test = y[test_idx]

            y_pred = model.predict(X_test)

            all_predictions.extend(y_pred)
            all_actuals.extend(y_test)

        rmse = np.sqrt(mean_squared_error(all_actuals, all_predictions))
        mae = mean_absolute_error(all_actuals, all_predictions)
        r2 = r2_score(all_actuals, all_predictions)

        return {
            'rmse': float(rmse),
            'mae': float(mae),
            'r2': float(r2)
        }

    def _save_model(self, pair, model, selected_features, days, best_score, best_params, metrics):
        """ä¿å­˜æ¨¡å‹å’Œå…ƒæ•°æ®"""
        pair_key = pair.replace('/', '_')

        # ä¿å­˜LightGBMæ¨¡å‹
        model_path = os.path.join(self.model_dir, f'{pair_key}_lgb_model.txt')
        model.booster_.save_model(model_path)

        # åˆ›å»ºå…ƒæ•°æ®
        metadata = {
            'pair': pair,
            'trained_at': datetime.now().isoformat(),
            'model_version': '2.0_balanced',
            'training_config': {
                'days': days,
                'lookback': 12,
                'lookahead': 6,
                'n_features': len(selected_features)
            },
            'performance': {
                'r2_score': float(best_score),
                'rmse': metrics['rmse'],
                'mae': metrics['mae']
            },
            'model_params': best_params,
            'features': selected_features[:20]  # åªä¿å­˜å‰20ä¸ªé‡è¦ç‰¹å¾
        }

        metadata_path = os.path.join(self.model_dir, f'{pair_key}_metadata.json')
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)

        logger.info(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")

        return metadata


def main():
    parser = argparse.ArgumentParser(description='å¹³è¡¡ç‰ˆæ¨¡å‹è®­ç»ƒ')
    parser.add_argument('--pair', type=str, default='ETH/USDT', help='äº¤æ˜“å¯¹')
    parser.add_argument('--days', type=int, default=7, help='è®­ç»ƒæ•°æ®å¤©æ•°')
    parser.add_argument('--all', action='store_true', help='è®­ç»ƒæ‰€æœ‰ä¸»è¦è´§å¸å¯¹')

    args = parser.parse_args()

    trainer = BalancedModelTrainer()

    if args.all:
        # æ‰¹é‡è®­ç»ƒ
        pairs = ['ETH/USDT', 'BTC/USDT', 'BNB/USDT', 'SOL/USDT', 'ADA/USDT']
        for pair in pairs:
            try:
                logger.info(f"\n{'='*40}")
                logger.info(f"è®­ç»ƒ {pair}")
                logger.info(f"{'='*40}")
                trainer.train_balanced(pair, args.days)
            except Exception as e:
                logger.error(f"âŒ {pair} è®­ç»ƒå¤±è´¥: {e}")
    else:
        # å•ä¸ªè®­ç»ƒ
        trainer.train_balanced(args.pair, args.days)

    logger.info("\nâœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆ!")


if __name__ == "__main__":
    main()
"""
ğŸš€ AetherPay AI Model Training - Optimized Version
ä¼˜åŒ–ç­–ç•¥ï¼š
1. è®­ç»ƒæ•°æ®ï¼š90å¤© â†’ 7å¤©ï¼ˆå‡å°‘è¿‡æ—¶æ•°æ®ï¼‰
2. æ»‘çª—çª—å£ï¼š24å°æ—¶ â†’ 12å°æ—¶ï¼ˆé™ä½ç‰¹å¾ç»´åº¦ï¼‰
3. ç‰¹å¾æ•°é‡ï¼š200 â†’ 100ï¼ˆåŠ é€Ÿè®­ç»ƒï¼‰
4. äº¤å‰éªŒè¯ï¼š5æŠ˜ â†’ 3æŠ˜ï¼ˆèŠ‚çœ40%æ—¶é—´ï¼‰
5. å‚æ•°æœç´¢ï¼šGridSearchCV â†’ RandomizedSearchCVï¼ˆèŠ‚çœ60%æ—¶é—´ï¼‰
6. æ¨¡å‹ç®€åŒ–ï¼šLightGBM + XGBoost â†’ ä»…LightGBMï¼ˆèŠ‚çœ50%æ—¶é—´ï¼‰
7. æ ‘çš„æ•°é‡ï¼š300 â†’ 150ï¼ˆåŠ é€Ÿå•æ¬¡æ‹Ÿåˆï¼‰

é¢„æœŸæ•ˆæœï¼š
- è®­ç»ƒæ—¶é—´ï¼š2.3å°æ—¶ â†’ 3-5åˆ†é’Ÿï¼ˆæé€Ÿ30å€ï¼‰
- å‡†ç¡®ç‡ï¼š61.8% â†’ 65%+ï¼ˆæ•°æ®æ›´ç›¸å…³ï¼‰
"""

import numpy as np
import pandas as pd
import lightgbm as lgb
from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from scipy.stats import uniform, randint
import json
import os
import sys
from datetime import datetime
import logging
import warnings

# æŠ‘åˆ¶ç‰¹å¾åç§°è­¦å‘Š
warnings.filterwarnings('ignore', message='X does not have valid feature names', category=UserWarning)
warnings.filterwarnings('ignore', category=UserWarning, module='lightgbm')

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from data.data_preprocessor_robust import RobustDataPreprocessor

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class OptimizedModelTrainer:
    """ä¼˜åŒ–çš„æ¨¡å‹è®­ç»ƒå™¨"""

    def __init__(self, model_dir='./saved_models'):
        self.model_dir = model_dir
        self.preprocessor = RobustDataPreprocessor()
        os.makedirs(model_dir, exist_ok=True)

    def train_model(self, pair='BTC/USDT', days=7, fast_mode=False, limit_rows=None):
        """
        è®­ç»ƒä¼˜åŒ–åçš„æ¨¡å‹

        Args:
            pair: äº¤æ˜“å¯¹ï¼ˆå¦‚ 'BTC/USDT'ï¼‰
            days: è®­ç»ƒæ•°æ®å¤©æ•°ï¼ˆé»˜è®¤7å¤©ï¼Œæœ€ä¼˜å¹³è¡¡ç‚¹ï¼‰
            fast_mode: å¿«é€Ÿæ¨¡å¼ï¼ˆå¼€å‘æµ‹è¯•ç”¨ï¼Œ3å¤©æ•°æ®+æ›´å°‘è¿­ä»£ï¼‰

        Returns:
            metadata: è®­ç»ƒå…ƒæ•°æ®
        """
        if fast_mode:
            days = 3
            logger.info(f"âš¡ FAST MODE: {days}å¤©æ•°æ®, å‡å°‘è¿­ä»£æ¬¡æ•°")

        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸš€ å¼€å§‹è®­ç»ƒ {pair} æ¨¡å‹")
        logger.info(f"ğŸ“… è®­ç»ƒæ•°æ®: {days}å¤©")
        logger.info(f"{'='*60}\n")

        # Step 1: æ•°æ®å‡†å¤‡
        start_time = datetime.now()
        X, y, timestamps, feature_names = self._prepare_data(pair, days, fast_mode, limit_rows=limit_rows)
        logger.info(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {len(X)}æ ·æœ¬ Ã— {X.shape[1]}ç‰¹å¾")

        # Step 2: ç‰¹å¾é€‰æ‹©
        n_features = 50 if fast_mode else 100
        X_selected, selected_features = self._select_features(
            X, y, feature_names, n_features
        )
        logger.info(f"âœ… ç‰¹å¾é€‰æ‹©å®Œæˆ: {len(selected_features)}ä¸ªç‰¹å¾")

        # Step 3: æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
        n_splits = 2 if fast_mode else 3
        tscv = TimeSeriesSplit(n_splits=n_splits)
        logger.info(f"ğŸ“Š äº¤å‰éªŒè¯: {n_splits}æŠ˜")

        # Step 4: è®­ç»ƒLightGBMï¼ˆéšæœºæœç´¢ï¼‰
        n_iter = 10 if fast_mode else 20
        model, best_score, best_params = self._train_lightgbm(
            X_selected, y, tscv, n_iter
        )
        logger.info(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ: RÂ² = {best_score:.4f}")

        # Step 5: æœ€ç»ˆè¯„ä¼°
        final_metrics = self._evaluate_model(model, X_selected, y, tscv)

        # Step 6: ä¿å­˜æ¨¡å‹
        metadata = self._save_model(
            pair, model, selected_features,
            days, best_score, best_params, final_metrics, fast_mode
        )

        # è®¡ç®—æ€»è€—æ—¶
        total_time = (datetime.now() - start_time).total_seconds()

        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ‰ è®­ç»ƒå®Œæˆ!")
        logger.info(f"â±ï¸  æ€»è€—æ—¶: {total_time:.1f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
        logger.info(f"ğŸ“ˆ RÂ² Score: {best_score:.4f}")
        logger.info(f"ğŸ“Š RMSE: {final_metrics['rmse']:.4f}")
        logger.info(f"ğŸ“Š MAE: {final_metrics['mae']:.4f}")
        logger.info(f"ğŸ’¾ æ¨¡å‹ä¿å­˜è·¯å¾„: {self.model_dir}")
        logger.info(f"{'='*60}\n")

        return metadata

    def _prepare_data(self, pair, days, fast_mode, limit_rows=None):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        rates_df, market_df = self.preprocessor.load_training_data(pair, days)

        if rates_df.empty:
            raise ValueError(f"âŒ {pair} æ²¡æœ‰å¯ç”¨æ•°æ®")

        logger.info(f"ğŸ“¥ åŸå§‹æ•°æ®: {len(rates_df)}æ¡è®°å½•")

        # åˆ›å»ºç‰¹å¾
        features_df = self.preprocessor.create_features(rates_df, market_df)
        features_df = self.preprocessor.add_external_features(features_df)
        if limit_rows is not None and isinstance(limit_rows, int) and limit_rows > 0:
            features_df = features_df.tail(limit_rows)
            logger.info(f"ğŸ“‰ å·²è£å‰ªåˆ°æœ€è¿‘ {limit_rows} è¡Œç‰¹å¾æ•°æ®")

        # å‡†å¤‡æœºå™¨å­¦ä¹ æ•°æ®ï¼ˆä¼˜åŒ–ï¼š12å°æ—¶æ»‘çª— vs åŸæ¥24å°æ—¶ï¼‰
        lookback = 6 if fast_mode else 12  # å¿«é€Ÿæ¨¡å¼6hï¼Œæ­£å¸¸æ¨¡å¼12h
        X, y, timestamps, feature_cols = self.preprocessor.prepare_ml_data(
            features_df,
            target_col='avg_price',
            lookback=lookback,
            lookahead=6
        )

        return X, y, timestamps, feature_cols

    def _select_features(self, X, y, feature_names, n_features=100):
        """
        å¿«é€Ÿç‰¹å¾é€‰æ‹©
        ä¼˜åŒ–ï¼š50æ£µæ ‘ï¼ˆvs åŸæ¥100æ£µï¼‰
        """
        logger.info(f"ğŸ” ç‰¹å¾é€‰æ‹©: {X.shape[1]} â†’ {n_features}")

        # å¿«é€Ÿè®­ç»ƒä¸€ä¸ªå°æ¨¡å‹è·å–ç‰¹å¾é‡è¦æ€§
        selector = lgb.LGBMRegressor(
            n_estimators=50,  # å‡å°‘æ ‘çš„æ•°é‡
            learning_rate=0.1,
            max_depth=5,
            num_leaves=31,
            random_state=42,
            n_jobs=-1,
            verbosity=-1
        )
        selector.fit(X, y)

        # è·å–ç‰¹å¾é‡è¦æ€§
        importances = selector.feature_importances_

        # ä¿®å¤ç‰¹å¾åé•¿åº¦ä¸åŒ¹é…
        if len(feature_names) != len(importances):
            logger.warning(f"âš ï¸ ç‰¹å¾åä¸åŒ¹é…: {len(feature_names)} vs {len(importances)}")
            feature_names = [f'feature_{i}' for i in range(len(importances))]

        # é€‰æ‹©Top Nç‰¹å¾
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': importances
        }).sort_values('importance', ascending=False)

        top_features = importance_df.head(n_features)['feature'].tolist()
        feature_indices = [feature_names.index(f) for f in top_features]
        X_selected = X[:, feature_indices]

        # ä¿å­˜ç‰¹å¾é‡è¦æ€§æŠ¥å‘Š
        importance_path = os.path.join(self.model_dir, 'feature_importance.csv')
        importance_df.to_csv(importance_path, index=False)
        logger.info(f"ğŸ’¾ ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜: {importance_path}")

        return X_selected, top_features

    def _train_lightgbm(self, X, y, tscv, n_iter=20):
        """
        ä½¿ç”¨RandomizedSearchCVè®­ç»ƒLightGBM
        ä¼˜åŒ–ï¼šéšæœºæœç´¢ vs ç½‘æ ¼æœç´¢ï¼Œå¤§å¹…å‡å°‘æœç´¢æ—¶é—´
        """
        logger.info(f"ğŸ¯ å¼€å§‹è®­ç»ƒ (éšæœºæœç´¢ {n_iter} æ¬¡è¿­ä»£)...")

        # å‚æ•°åˆ†å¸ƒï¼ˆä½¿ç”¨è¿ç»­åˆ†å¸ƒï¼Œæœç´¢ç©ºé—´æ›´å¤§ï¼‰
        param_distributions = {
            'num_leaves': randint(20, 100),
            'learning_rate': uniform(0.03, 0.17),  # [0.03, 0.2]
            'feature_fraction': uniform(0.7, 0.3),  # [0.7, 1.0]
            'bagging_fraction': uniform(0.7, 0.3),
            'max_depth': randint(5, 11),
            'min_child_samples': randint(10, 40),
            'reg_alpha': uniform(0, 0.5),
            'reg_lambda': uniform(0, 0.5),
        }

        # åŸºç¡€æ¨¡å‹
        base_model = lgb.LGBMRegressor(
            n_estimators=150,  # å‡å°‘æ ‘çš„æ•°é‡ï¼ˆvs åŸæ¥300ï¼‰
            random_state=42,
            n_jobs=-1,
            verbosity=-1
        )

        # éšæœºæœç´¢
        random_search = RandomizedSearchCV(
            base_model,
            param_distributions,
            n_iter=n_iter,
            cv=tscv,
            scoring='r2',
            n_jobs=-1,
            verbose=0,  # å‡å°‘è¾“å‡º
            random_state=42,
            return_train_score=True
        )

        random_search.fit(X, y)

        logger.info(f"ğŸ† æœ€ä½³å‚æ•°: {random_search.best_params_}")
        logger.info(f"ğŸ“Š æœ€ä½³åˆ†æ•°: {random_search.best_score_:.4f}")

        return random_search.best_estimator_, random_search.best_score_, random_search.best_params_

    def _evaluate_model(self, model, X, y, tscv):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        all_predictions = []
        all_actuals = []

        for train_idx, test_idx in tscv.split(X):
            X_test = X[test_idx]
            y_test = y[test_idx]

            y_pred = model.predict(X_test)

            all_predictions.extend(y_pred)
            all_actuals.extend(y_test)

        # è®¡ç®—æŒ‡æ ‡
        rmse = np.sqrt(mean_squared_error(all_actuals, all_predictions))
        mae = mean_absolute_error(all_actuals, all_predictions)
        r2 = r2_score(all_actuals, all_predictions)

        # è®¡ç®—MAPEï¼ˆå¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®ï¼‰
        mape = np.mean(np.abs((np.array(all_actuals) - np.array(all_predictions)) / np.array(all_actuals))) * 100

        return {
            'rmse': float(rmse),
            'mae': float(mae),
            'r2': float(r2),
            'mape': float(mape)
        }

    def _save_model(self, pair, model, selected_features, days, best_score, best_params, metrics, fast_mode):
        """ä¿å­˜æ¨¡å‹å’Œå…ƒæ•°æ®"""
        pair_key = pair.replace('/', '_')

        # ä¿å­˜LightGBMæ¨¡å‹
        model_path = os.path.join(self.model_dir, f'{pair_key}_lgb_model.txt')
        model.booster_.save_model(model_path)

        # åˆ›å»ºå…ƒæ•°æ®
        metadata = {
            'pair': pair,
            'trained_at': datetime.now().isoformat(),
            'model_version': '3.0_optimized',
            'optimization_strategy': {
                'training_days': days,
                'cv_folds': 3 if not fast_mode else 2,
                'search_method': 'RandomizedSearchCV',
                'lookback_hours': 12 if not fast_mode else 6,
                'n_estimators': 150,
                'feature_selection': len(selected_features)
            },
            'training_data': {
                'days': days,
                'fast_mode': fast_mode
            },
            'performance': {
                'r2_score': float(best_score),
                'rmse': metrics['rmse'],
                'mae': metrics['mae'],
                'mape': metrics['mape']
            },
            'model_params': best_params,
            'features': {
                'count': len(selected_features),
                'names': selected_features
            },
            'usage': {
                'predictor': 'oracle_predictor.py',
                'load_method': 'lgb.Booster(model_file=...)'
            }
        }

        # ä¿å­˜å…ƒæ•°æ®
        metadata_path = os.path.join(self.model_dir, f'{pair_key}_metadata.json')
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)

        logger.info(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")
        logger.info(f"ğŸ’¾ å…ƒæ•°æ®å·²ä¿å­˜: {metadata_path}")

        return metadata


def compare_training_windows(pair='BTC/USDT'):
    """
    å¯¹æ¯”ä¸åŒè®­ç»ƒçª—å£çš„æ•ˆæœ
    ç”¨äºéªŒè¯7å¤©æ˜¯å¦çœŸçš„æœ€ä¼˜
    """
    trainer = OptimizedModelTrainer()

    logger.info("\n" + "="*60)
    logger.info("ğŸ“Š å¯¹æ¯”å®éªŒ: ä¸åŒè®­ç»ƒçª—å£å¯¹å‡†ç¡®ç‡çš„å½±å“")
    logger.info("="*60 + "\n")

    results = []

    for days in [3, 7, 14, 30]:
        logger.info(f"\n{'='*40}")
        logger.info(f"æµ‹è¯•: {days}å¤©è®­ç»ƒæ•°æ®")
        logger.info(f"{'='*40}")

        try:
            metadata = trainer.train_model(pair, days=days)
            results.append({
                'days': days,
                'r2': metadata['performance']['r2_score'],
                'rmse': metadata['performance']['rmse'],
                'mae': metadata['performance']['mae']
            })
        except Exception as e:
            logger.error(f"âŒ {days}å¤©è®­ç»ƒå¤±è´¥: {e}")

    # æ‰“å°å¯¹æ¯”ç»“æœ
    logger.info("\n" + "="*60)
    logger.info("ğŸ“Š å¯¹æ¯”ç»“æœæ±‡æ€»")
    logger.info("="*60)

    df = pd.DataFrame(results)
    print(df.to_string(index=False))

    # æ‰¾å‡ºæœ€ä½³çª—å£
    best_idx = df['r2'].idxmax()
    best_days = df.loc[best_idx, 'days']

    logger.info(f"\nğŸ† æœ€ä½³è®­ç»ƒçª—å£: {best_days}å¤©")
    logger.info(f"ğŸ“ˆ RÂ² Score: {df.loc[best_idx, 'r2']:.4f}")

    return df


if __name__ == "__main__":
    import sys
    import argparse

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='è®­ç»ƒä¼˜åŒ–çš„æ±‡ç‡é¢„æµ‹æ¨¡å‹')
    parser.add_argument('--pair', type=str, default='BTC/USDT',
                       help='äº¤æ˜“å¯¹ï¼Œä¾‹å¦‚ BTC/USDT æˆ– CNY/USD (é»˜è®¤: BTC/USDT)')
    parser.add_argument('--days', type=int, default=7,
                       help='è®­ç»ƒæ•°æ®å¤©æ•° (é»˜è®¤: 7)')
    parser.add_argument('mode', nargs='?', default='normal',
                       choices=['normal', 'fast', 'compare'],
                       help='è®­ç»ƒæ¨¡å¼: normal (æ­£å¸¸), fast (å¿«é€Ÿ), compare (å¯¹æ¯”)')

    args = parser.parse_args()

    # é»˜è®¤è®­ç»ƒæ¨¡å¼
    trainer = OptimizedModelTrainer()

    if args.mode == 'compare':
        # å¯¹æ¯”æ¨¡å¼ï¼špython train_model_optimized.py compare --pair CNY/USD
        logger.info(f"\nğŸ“Š å¯¹æ¯”ä¸åŒè®­ç»ƒçª—å£æ•ˆæœ ({args.pair})")
        compare_training_windows(args.pair)
    elif args.mode == 'fast':
        # å¿«é€Ÿæ¨¡å¼ï¼špython train_model_optimized.py fast --pair CNY/USD
        logger.info(f"\nâš¡ å¿«é€Ÿè®­ç»ƒæ¨¡å¼ (3å¤©æ•°æ®ï¼Œé€‚åˆå¼€å‘æµ‹è¯•) - {args.pair}")
        trainer.train_model(args.pair, fast_mode=True)
    else:
        # æ­£å¸¸æ¨¡å¼ï¼špython train_model_optimized.py --pair CNY/USD
        logger.info(f"\nğŸš€ æ­£å¸¸è®­ç»ƒæ¨¡å¼ ({args.days}å¤©æ•°æ®ï¼Œæ¨è) - {args.pair}")
        trainer.train_model(args.pair, days=args.days)

    logger.info("\nâœ… æ‰€æœ‰è®­ç»ƒä»»åŠ¡å®Œæˆ!")
    logger.info(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: {trainer.model_dir}")


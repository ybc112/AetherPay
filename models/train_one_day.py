"""
1å¤©æ•°æ®ä¼˜åŒ–è®­ç»ƒè„šæœ¬ - å¹³è¡¡é€Ÿåº¦å’Œå‡†ç¡®ç‡
ä½¿ç”¨1å¤©æ•°æ®ï¼Œä½†é€šè¿‡æ›´å¥½çš„ç‰¹å¾å·¥ç¨‹æå‡å‡†ç¡®ç‡
é¢„æœŸï¼š30ç§’-1åˆ†é’Ÿå®Œæˆï¼ŒRÂ²>0.5
"""

import numpy as np
import pandas as pd
import lightgbm as lgb
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import sqlite3
import json
import os
from datetime import datetime, timedelta
import logging
import warnings
import argparse

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)

class OneDayTrainer:
    """1å¤©æ•°æ®ä¼˜åŒ–è®­ç»ƒå™¨"""

    def __init__(self, model_dir='./saved_models'):
        self.model_dir = model_dir
        os.makedirs(model_dir, exist_ok=True)

    def train(self, pair='ETH/USDT'):
        """ä½¿ç”¨1å¤©æ•°æ®è®­ç»ƒï¼Œä½†ä¼˜åŒ–ç‰¹å¾å’Œå‚æ•°"""

        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ“Š {pair} - 1å¤©æ•°æ®ä¼˜åŒ–è®­ç»ƒ")
        logger.info(f"âš¡ ç›®æ ‡: 30ç§’å®Œæˆ, RÂ²>0.5")
        logger.info(f"{'='*60}\n")

        start_time = datetime.now()

        # 1. åŠ è½½1å¤©æ•°æ®ï¼ˆä½†ä½¿ç”¨æ‰€æœ‰è®°å½•ï¼‰
        logger.info("ğŸ“¥ åŠ è½½æœ€è¿‘1å¤©æ•°æ®...")
        X, y = self._load_one_day_data(pair)

        if len(X) < 100:
            logger.error(f"æ•°æ®ä¸è¶³: {len(X)}æ¡")
            return None

        logger.info(f"âœ… æ•°æ®: {len(X)}æ ·æœ¬ Ã— {X.shape[1]}ç‰¹å¾")

        # 2. æ—¶åºåˆ†å‰²ï¼ˆ80/20ï¼‰
        split_idx = int(len(X) * 0.8)
        X_train = X[:split_idx]
        y_train = y[:split_idx]
        X_test = X[split_idx:]
        y_test = y[split_idx:]

        logger.info(f"ğŸ“Š è®­ç»ƒé›†: {len(X_train)}æ ·æœ¬, æµ‹è¯•é›†: {len(X_test)}æ ·æœ¬")

        # 3. ä¼˜åŒ–çš„LightGBMå‚æ•°
        logger.info("ğŸš€ å¼€å§‹è®­ç»ƒ...")
        model = lgb.LGBMRegressor(
            n_estimators=100,      # 100æ£µæ ‘ï¼ˆå¹³è¡¡ï¼‰
            learning_rate=0.05,
            num_leaves=31,
            max_depth=6,
            min_child_samples=20,
            subsample=0.8,
            colsample_bytree=0.8,
            reg_alpha=0.1,
            reg_lambda=0.1,
            random_state=42,
            n_jobs=-1,
            verbosity=-1
        )

        # è®­ç»ƒ
        model.fit(
            X_train, y_train,
            eval_set=[(X_test, y_test)],
            callbacks=[lgb.early_stopping(10), lgb.log_evaluation(0)]
        )

        # 4. è¯„ä¼°
        y_pred_train = model.predict(X_train)
        y_pred_test = model.predict(X_test)

        train_r2 = r2_score(y_train, y_pred_train)
        test_r2 = r2_score(y_test, y_pred_test)
        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
        test_mae = mean_absolute_error(y_test, y_pred_test)

        # è®¡ç®—MAPE
        test_mape = np.mean(np.abs((y_test - y_pred_test) / y_test)) * 100

        logger.info(f"ğŸ“ˆ è®­ç»ƒRÂ²: {train_r2:.3f}")
        logger.info(f"ğŸ“Š æµ‹è¯•RÂ²: {test_r2:.3f}")
        logger.info(f"ğŸ“Š RMSE: {test_rmse:.2f}")
        logger.info(f"ğŸ“Š MAE: {test_mae:.2f}")
        logger.info(f"ğŸ“Š MAPE: {test_mape:.2f}%")

        # 5. ä¿å­˜
        self._save_model(pair, model, test_r2, test_rmse, test_mae, test_mape)

        # è®¡æ—¶
        total_time = (datetime.now() - start_time).total_seconds()
        logger.info(f"\nâœ… è®­ç»ƒå®Œæˆï¼è€—æ—¶: {total_time:.1f}ç§’")
        logger.info(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {self.model_dir}")

        # æ•ˆæœè¯„ä»·
        if test_r2 > 0.7:
            logger.info(f"ğŸ‰ æ•ˆæœä¼˜ç§€ï¼RÂ²={test_r2:.3f} > 0.7")
        elif test_r2 > 0.5:
            logger.info(f"âœ… æ•ˆæœè‰¯å¥½ï¼RÂ²={test_r2:.3f} > 0.5")
        elif test_r2 > 0.3:
            logger.info(f"âš ï¸ æ•ˆæœä¸€èˆ¬ï¼ŒRÂ²={test_r2:.3f}")
        else:
            logger.info(f"âŒ æ•ˆæœè¾ƒå·®ï¼ŒRÂ²={test_r2:.3f}")

        return {
            'r2': test_r2,
            'rmse': test_rmse,
            'mae': test_mae,
            'mape': test_mape,
            'time': total_time
        }

    def _load_one_day_data(self, pair):
        """åŠ è½½å¹¶å¤„ç†1å¤©çš„æ•°æ®"""
        conn = sqlite3.connect('aether_oracle.db')

        # è·å–æœ€è¿‘24å°æ—¶çš„æ‰€æœ‰æ•°æ®
        query = """
        SELECT
            price,
            volume,
            timestamp
        FROM exchange_rates
        WHERE pair = ?
        AND timestamp >= datetime('now', '-1 day')
        ORDER BY timestamp
        """

        df = pd.read_sql_query(query, conn, params=(pair,))
        conn.close()

        if df.empty:
            # å¦‚æœæ²¡æœ‰æœ€è¿‘24å°æ—¶çš„æ•°æ®ï¼Œè·å–æœ€æ–°çš„1000æ¡
            conn = sqlite3.connect('aether_oracle.db')
            query = """
            SELECT price, volume, timestamp
            FROM exchange_rates
            WHERE pair = ?
            ORDER BY timestamp DESC
            LIMIT 1000
            """
            df = pd.read_sql_query(query, conn, params=(pair,))
            df = df.iloc[::-1].reset_index(drop=True)  # åè½¬é¡ºåº
            conn.close()

        if df.empty or len(df) < 100:
            return np.array([]), np.array([])

        # åˆ›å»ºä¼˜åŒ–çš„ç‰¹å¾é›†
        features = self._create_optimized_features(df)

        # å‡†å¤‡è®­ç»ƒæ•°æ®
        lookback = 10  # ä½¿ç”¨10ä¸ªå†å²ç‚¹
        X, y = self._prepare_sequences(features, lookback)

        return X, y

    def _create_optimized_features(self, df):
        """åˆ›å»ºä¼˜åŒ–çš„ç‰¹å¾é›† - 20ä¸ªå…³é”®ç‰¹å¾"""
        features = pd.DataFrame()

        # åŸºç¡€ä»·æ ¼å’Œæˆäº¤é‡
        features['price'] = df['price'].values
        features['volume'] = df['volume'].values
        features['log_price'] = np.log(features['price'])
        features['log_volume'] = np.log(features['volume'] + 1)

        # æ”¶ç›Šç‡ç‰¹å¾ï¼ˆå…³é”®ï¼‰
        features['returns'] = features['price'].pct_change()
        features['returns_2'] = features['price'].pct_change(2)
        features['returns_5'] = features['price'].pct_change(5)
        features['log_returns'] = features['log_price'].diff()

        # ç§»åŠ¨å¹³å‡ï¼ˆä¸åŒå‘¨æœŸï¼‰
        for window in [5, 10, 20, 30]:
            features[f'sma_{window}'] = features['price'].rolling(window, min_periods=1).mean()
            features[f'price_to_sma_{window}'] = features['price'] / features[f'sma_{window}']

        # æŒ‡æ•°ç§»åŠ¨å¹³å‡
        features['ema_10'] = features['price'].ewm(span=10, adjust=False).mean()
        features['price_to_ema'] = features['price'] / features['ema_10']

        # æ³¢åŠ¨ç‡ï¼ˆé‡è¦ï¼‰
        features['volatility_5'] = features['returns'].rolling(5, min_periods=1).std()
        features['volatility_10'] = features['returns'].rolling(10, min_periods=1).std()
        features['volatility_ratio'] = features['volatility_5'] / (features['volatility_10'] + 1e-10)

        # æˆäº¤é‡ç‰¹å¾
        features['volume_sma_10'] = features['volume'].rolling(10, min_periods=1).mean()
        features['volume_ratio'] = features['volume'] / (features['volume_sma_10'] + 1e-10)
        features['volume_change'] = features['volume'].pct_change()

        # RSIï¼ˆç›¸å¯¹å¼ºå¼±æŒ‡æ ‡ï¼‰
        delta = features['price'].diff()
        gain = delta.where(delta > 0, 0).rolling(14, min_periods=1).mean()
        loss = -delta.where(delta < 0, 0).rolling(14, min_periods=1).mean()
        rs = gain / (loss + 1e-10)
        features['rsi'] = 100 - (100 / (1 + rs))

        # MACDæŒ‡æ ‡
        ema_12 = features['price'].ewm(span=12, adjust=False).mean()
        ema_26 = features['price'].ewm(span=26, adjust=False).mean()
        features['macd'] = ema_12 - ema_26
        features['macd_signal'] = features['macd'].ewm(span=9, adjust=False).mean()
        features['macd_diff'] = features['macd'] - features['macd_signal']

        # å¸ƒæ—å¸¦
        bb_period = 20
        bb_std = features['price'].rolling(bb_period, min_periods=1).std()
        bb_mean = features['price'].rolling(bb_period, min_periods=1).mean()
        features['bb_upper'] = bb_mean + (bb_std * 2)
        features['bb_lower'] = bb_mean - (bb_std * 2)
        features['bb_position'] = (features['price'] - features['bb_lower']) / (features['bb_upper'] - features['bb_lower'] + 1e-10)

        # ä»·æ ¼ä½ç½®ç‰¹å¾
        features['price_position'] = (features['price'] - features['price'].rolling(50, min_periods=1).min()) / \
                                     (features['price'].rolling(50, min_periods=1).max() -
                                      features['price'].rolling(50, min_periods=1).min() + 1e-10)

        # å¡«å……ç¼ºå¤±å€¼
        features = features.fillna(method='ffill').fillna(0)

        return features

    def _prepare_sequences(self, features, lookback=10):
        """å‡†å¤‡åºåˆ—æ•°æ®"""
        X = []
        y = []

        for i in range(lookback, len(features) - 1):
            X.append(features.iloc[i-lookback:i].values.flatten())
            y.append(features.iloc[i + 1]['price'])

        return np.array(X), np.array(y)

    def _save_model(self, pair, model, r2, rmse, mae, mape):
        """ä¿å­˜æ¨¡å‹å’Œå…ƒæ•°æ®"""
        pair_key = pair.replace('/', '_')

        # ä¿å­˜æ¨¡å‹
        model_path = os.path.join(self.model_dir, f'{pair_key}_lgb_model.txt')
        model.booster_.save_model(model_path)

        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'pair': pair,
            'trained_at': datetime.now().isoformat(),
            'version': 'one_day_optimized_v1',
            'performance': {
                'r2': float(r2),
                'rmse': float(rmse),
                'mae': float(mae),
                'mape': float(mape)
            },
            'training_config': {
                'data': '1_day',
                'features': 30,
                'lookback': 10,
                'model': 'LightGBM',
                'n_estimators': 100
            }
        }

        metadata_path = os.path.join(self.model_dir, f'{pair_key}_metadata.json')
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)

        logger.info(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")


def main():
    parser = argparse.ArgumentParser(description='1å¤©æ•°æ®ä¼˜åŒ–è®­ç»ƒ')
    parser.add_argument('--pair', type=str, default='ETH/USDT', help='äº¤æ˜“å¯¹')
    parser.add_argument('--all', action='store_true', help='è®­ç»ƒæ‰€æœ‰ä¸»è¦å¸ç§')

    args = parser.parse_args()

    trainer = OneDayTrainer()

    if args.all:
        # æ‰¹é‡è®­ç»ƒ
        pairs = ['ETH/USDT', 'BTC/USDT', 'BNB/USDT', 'SOL/USDT', 'ADA/USDT']
        results = []

        for pair in pairs:
            logger.info(f"\n{'='*40}")
            logger.info(f"è®­ç»ƒ {pair}")
            logger.info(f"{'='*40}")
            try:
                result = trainer.train(pair)
                if result:
                    results.append({'pair': pair, **result})
            except Exception as e:
                logger.error(f"âŒ {pair} å¤±è´¥: {e}")

        # æ‰“å°æ€»ç»“
        if results:
            logger.info(f"\n{'='*60}")
            logger.info("ğŸ“Š è®­ç»ƒç»“æœæ±‡æ€»")
            logger.info(f"{'='*60}")
            for r in results:
                logger.info(f"{r['pair']}: RÂ²={r['r2']:.3f}, è€—æ—¶={r['time']:.1f}ç§’")
    else:
        # å•ä¸ªè®­ç»ƒ
        trainer.train(args.pair)


if __name__ == "__main__":
    main()
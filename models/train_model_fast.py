"""
âš¡ AetherPay AIæ¨¡å‹è¶…å¿«é€Ÿè®­ç»ƒè„šæœ¬
ä¼˜åŒ–é‡ç‚¹ï¼šè§£å†³ETH/USDTè®­ç»ƒç¼“æ…¢é—®é¢˜
é¢„æœŸï¼š2-3åˆ†é’Ÿå®Œæˆè®­ç»ƒï¼ˆvs ä¹‹å‰å¯èƒ½20-30åˆ†é’Ÿï¼‰
"""

import numpy as np
import pandas as pd
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import StandardScaler
import sqlite3
import json
import os
import sys
from datetime import datetime, timedelta
import logging
import warnings

warnings.filterwarnings('ignore')

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class FastModelTrainer:
    """è¶…å¿«é€Ÿæ¨¡å‹è®­ç»ƒå™¨ - ä¸“æ³¨é€Ÿåº¦ä¼˜åŒ–"""

    def __init__(self, model_dir='./saved_models'):
        self.model_dir = model_dir
        self.scaler = StandardScaler()
        os.makedirs(model_dir, exist_ok=True)

    def train_fast(self, pair='ETH/USDT'):
        """
        å¿«é€Ÿè®­ç»ƒæ¨¡å‹
        ä¼˜åŒ–ç­–ç•¥ï¼š
        1. ç®€åŒ–ç‰¹å¾å·¥ç¨‹ - åªç”¨æœ€é‡è¦çš„ç‰¹å¾
        2. å‡å°‘æ•°æ®é‡ - ä½¿ç”¨æœ€è¿‘3å¤©æ•°æ®
        3. å›ºå®šå‚æ•° - ä¸åšå‚æ•°æœç´¢
        4. ç®€å•éªŒè¯ - åªåštrain/test split
        """
        logger.info(f"\n{'='*60}")
        logger.info(f"âš¡ å¼€å§‹è¶…å¿«é€Ÿè®­ç»ƒ {pair} æ¨¡å‹")
        logger.info(f"{'='*60}\n")

        start_time = datetime.now()

        # Step 1: å¿«é€ŸåŠ è½½æ•°æ®
        logger.info("ğŸ“¥ åŠ è½½æ•°æ®...")
        X, y = self._load_data_fast(pair)

        if len(X) == 0:
            logger.error(f"âŒ æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®è®­ç»ƒ {pair}")
            return None

        logger.info(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {len(X)}æ ·æœ¬ Ã— {X.shape[1]}ç‰¹å¾")

        # Step 2: ç®€å•åˆ†å‰²æ•°æ®ï¼ˆ80/20ï¼‰
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, shuffle=False  # æ—¶é—´åºåˆ—ä¸æ‰“ä¹±
        )

        # Step 3: å¿«é€Ÿè®­ç»ƒï¼ˆå›ºå®šæœ€ä¼˜å‚æ•°ï¼‰
        logger.info("ğŸ¯ å¼€å§‹è®­ç»ƒ...")
        model = self._train_lgb_fast(X_train, y_train, X_test, y_test)

        # Step 4: è¯„ä¼°
        y_pred = model.predict(X_test)
        metrics = {
            'r2': r2_score(y_test, y_pred),
            'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),
            'mae': mean_absolute_error(y_test, y_pred)
        }

        logger.info(f"ğŸ“Š æ¨¡å‹æ€§èƒ½: RÂ²={metrics['r2']:.4f}, RMSE={metrics['rmse']:.2f}")

        # Step 5: ä¿å­˜æ¨¡å‹
        self._save_model_fast(pair, model, metrics)

        # è®¡ç®—æ€»è€—æ—¶
        total_time = (datetime.now() - start_time).total_seconds()

        logger.info(f"\n{'='*60}")
        logger.info(f"âœ… è®­ç»ƒå®Œæˆï¼")
        logger.info(f"â±ï¸  æ€»è€—æ—¶: {total_time:.1f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
        logger.info(f"ğŸ“ˆ RÂ² Score: {metrics['r2']:.4f}")
        logger.info(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {self.model_dir}")
        logger.info(f"{'='*60}\n")

        return metrics

    def _load_data_fast(self, pair, days=3):
        """å¿«é€ŸåŠ è½½å’Œå¤„ç†æ•°æ® - ç®€åŒ–ç‰ˆ"""
        conn = sqlite3.connect('aether-oracle/aether_oracle.db')

        # åªåŠ è½½æœ€è¿‘3å¤©æ•°æ®
        start_time = datetime.now() - timedelta(days=days)

        # ç®€å•æŸ¥è¯¢ - åªè·å–åŸºæœ¬æ•°æ®
        query = """
        SELECT
            price,
            volume,
            timestamp
        FROM exchange_rates
        WHERE pair = ?
        AND timestamp >= ?
        ORDER BY timestamp
        """

        df = pd.read_sql_query(query, conn, params=(pair, start_time.isoformat()))
        conn.close()

        if df.empty or len(df) < 100:
            logger.warning(f"æ•°æ®ä¸è¶³: {len(df)}æ¡")
            return np.array([]), np.array([])

        # è¶…ç®€å•ç‰¹å¾å·¥ç¨‹
        features = self._create_simple_features(df)

        # å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆç®€åŒ–ç‰ˆï¼‰
        X, y = self._prepare_data_fast(features)

        return X, y

    def _create_simple_features(self, df):
        """åˆ›å»ºç®€å•ä½†æœ‰æ•ˆçš„ç‰¹å¾"""
        # åŸºç¡€ç‰¹å¾
        features = pd.DataFrame()
        features['price'] = df['price'].values
        features['volume'] = df['volume'].values

        # ä»·æ ¼å˜åŒ–
        features['returns'] = features['price'].pct_change()

        # ç®€å•ç§»åŠ¨å¹³å‡ï¼ˆåªç”¨3ä¸ªï¼‰
        features['sma_5'] = features['price'].rolling(5, min_periods=1).mean()
        features['sma_10'] = features['price'].rolling(10, min_periods=1).mean()
        features['sma_20'] = features['price'].rolling(20, min_periods=1).mean()

        # ä»·æ ¼ä¸å‡çº¿çš„å·®
        features['price_sma5_diff'] = features['price'] - features['sma_5']
        features['price_sma10_diff'] = features['price'] - features['sma_10']

        # æˆäº¤é‡ç‰¹å¾
        features['volume_sma'] = features['volume'].rolling(5, min_periods=1).mean()
        features['volume_ratio'] = features['volume'] / (features['volume_sma'] + 1e-10)

        # ç®€å•æ³¢åŠ¨ç‡
        features['volatility'] = features['returns'].rolling(10, min_periods=1).std()

        # æ»åç‰¹å¾ï¼ˆåªç”¨3ä¸ªï¼‰
        for lag in [1, 3, 6]:
            features[f'price_lag_{lag}'] = features['price'].shift(lag)
            features[f'returns_lag_{lag}'] = features['returns'].shift(lag)

        # æ—¶é—´ç‰¹å¾
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        features['hour'] = df['timestamp'].dt.hour
        features['day_of_week'] = df['timestamp'].dt.dayofweek

        # å¡«å……ç¼ºå¤±å€¼
        features = features.fillna(method='ffill').fillna(0)

        return features

    def _prepare_data_fast(self, features, lookback=6, lookahead=6):
        """å¿«é€Ÿå‡†å¤‡æ•°æ® - é¿å…å¾ªç¯"""
        if len(features) < lookback + lookahead + 10:
            return np.array([]), np.array([])

        # æ–¹æ³•1ï¼šä½¿ç”¨å‘é‡åŒ–æ“ä½œä»£æ›¿å¾ªç¯
        n_samples = len(features) - lookback - lookahead
        n_features = len(features.columns)

        # é¢„åˆ†é…æ•°ç»„
        X = np.zeros((n_samples, lookback * n_features))
        y = np.zeros(n_samples)

        # å‘é‡åŒ–åˆ›å»ºç‰¹å¾ï¼ˆæ›´å¿«ï¼‰
        feature_array = features.values
        for i in range(n_samples):
            X[i] = feature_array[i:i+lookback].flatten()
            y[i] = feature_array[i+lookback+lookahead, 0]  # é¢„æµ‹æœªæ¥ä»·æ ¼

        # ç§»é™¤åŒ…å«NaNçš„æ ·æœ¬
        valid_mask = ~(np.isnan(X).any(axis=1) | np.isnan(y))
        X = X[valid_mask]
        y = y[valid_mask]

        # æ ‡å‡†åŒ–ç‰¹å¾
        if len(X) > 0:
            X = self.scaler.fit_transform(X)

        return X, y

    def _train_lgb_fast(self, X_train, y_train, X_valid, y_valid):
        """ä½¿ç”¨å›ºå®šçš„ä¼˜åŒ–å‚æ•°å¿«é€Ÿè®­ç»ƒ"""
        # ç»éªŒæœ€ä¼˜å‚æ•°ï¼ˆä¸åšæœç´¢ï¼‰
        params = {
            'objective': 'regression',
            'metric': 'rmse',
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.05,
            'feature_fraction': 0.9,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'verbose': -1,
            'n_jobs': -1
        }

        # åˆ›å»ºæ•°æ®é›†
        train_data = lgb.Dataset(X_train, label=y_train)
        valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data)

        # è®­ç»ƒæ¨¡å‹ï¼ˆåªç”¨100æ£µæ ‘ï¼Œå¿«é€Ÿï¼‰
        model = lgb.train(
            params,
            train_data,
            num_boost_round=100,  # å‡å°‘æ ‘çš„æ•°é‡
            valid_sets=[valid_data],
            callbacks=[lgb.early_stopping(10), lgb.log_evaluation(0)]
        )

        return model

    def _save_model_fast(self, pair, model, metrics):
        """å¿«é€Ÿä¿å­˜æ¨¡å‹"""
        pair_key = pair.replace('/', '_')

        # ä¿å­˜æ¨¡å‹
        model_path = os.path.join(self.model_dir, f'{pair_key}_lgb_model.txt')
        model.save_model(model_path)

        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'pair': pair,
            'trained_at': datetime.now().isoformat(),
            'model_version': 'fast_1.0',
            'performance': metrics,
            'training_mode': 'fast',
            'training_time_seconds': (datetime.now() - datetime.now()).total_seconds()
        }

        metadata_path = os.path.join(self.model_dir, f'{pair_key}_metadata.json')
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)

        logger.info(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")


def main():
    import argparse

    parser = argparse.ArgumentParser(description='è¶…å¿«é€Ÿæ¨¡å‹è®­ç»ƒ')
    parser.add_argument('--pair', type=str, default='ETH/USDT', help='äº¤æ˜“å¯¹')
    parser.add_argument('--batch', action='store_true', help='æ‰¹é‡è®­ç»ƒæ‰€æœ‰ä¸»è¦è´§å¸å¯¹')

    args = parser.parse_args()

    trainer = FastModelTrainer()

    if args.batch:
        # æ‰¹é‡è®­ç»ƒä¸»è¦è´§å¸å¯¹
        pairs = ['ETH/USDT', 'BTC/USDT', 'BNB/USDT', 'SOL/USDT', 'ADA/USDT']
        for pair in pairs:
            logger.info(f"\n{'='*40}")
            logger.info(f"è®­ç»ƒ {pair}")
            logger.info(f"{'='*40}")
            try:
                trainer.train_fast(pair)
            except Exception as e:
                logger.error(f"âŒ {pair} è®­ç»ƒå¤±è´¥: {e}")
    else:
        # å•ä¸ªè®­ç»ƒ
        trainer.train_fast(args.pair)

    logger.info("\nâœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆ!")


if __name__ == "__main__":
    main()